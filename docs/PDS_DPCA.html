<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>PDS_DPCA API documentation</title>
<meta name="description" content="A module for customized PCA class for dynamic process monitoring.
Monitoring methodology is described in our book
&#39;Machine Learning for Process â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>PDS_DPCA</code></h1>
</header>
<section id="section-intro">
<p>A module for customized PCA class for dynamic process monitoring.
Monitoring methodology is described in our book
'Machine Learning for Process Systems Engineering' (<a href="https://mlforpse.com/books/">https://mlforpse.com/books/</a>)
============================================================================</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
A module for customized PCA class for dynamic process monitoring.
Monitoring methodology is described in our book  &#39;Machine Learning for Process Systems Engineering&#39; (https://mlforpse.com/books/)
============================================================================
&#34;&#34;&#34;

# Version: 2023
# Author: Ankur Kumar @ MLforPSE.com
# License: BSD 3 clause

#%% Imports
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats

#%% utility functions
def _augument(X, n_lags):
    &#34;&#34;&#34; generate augumented matrix with n_lags lagged measurements for each feature
    
    Parameters:
    ---------------------
    X: ndarray of shape (n_samples, n_features)
        n_samples is the number of samples and n_features is the number of features.
    
    n_lags: The number of lags to be used for data augumentation.   
    
    
    Returns:
    ---------------------
    X_augmented: ndarray of shape (n_samples-n_lags, (n_lags+1)*n_features).
        The n_lags+1 values of feature j go into columns (j-1)*(n_lags+1) to j*(n_lags+1)-1.
        The ith row of X_augmented contains data from row i to row i+n_lags from matrix X.
    
    &#34;&#34;&#34;
    
    # sanity check (data matrix must have atleast n_lags+1 samples)
    if X.shape[0] &lt; n_lags+1:
        raise ValueError(&#34;Data matrix must have atleast n_lags+1 samples&#34;)
    
    # augment training data
    N, m = X.shape
    X_augmented = np.zeros((N-n_lags, (n_lags+1)*m))
    
    for sample in range(n_lags, N):
        XBlock = X[sample-n_lags:sample+1,:]
        X_augmented[sample-n_lags,:] = np.reshape(XBlock, (1,-1), order = &#39;F&#39;)
        
    return X_augmented


#%% define class
class PDS_DPCA(PCA):
    &#39;&#39;&#39; This class wraps Sklearn&#39;s PCA class to provide the following additional methods
    
    1) computeMetrics: computes the monitoring indices for the supplied data
    2) computeThresholds: computes the thresholds / control limits for the monitoring indices from training data
    3) draw_monitoring_charts: Draw the monitoring charts for the training or test data
    4) detect_abnormalities: Detects if the observations are abnormal or normal samples
    
    Additional Parameters
    ----------
    n_lags: integer (default 1)
        The number of lags to be used for data augumentation.
        
    
    Additional Attributes
    ----------
    n_lags: integer
        The number of lags used for data augumentation.
        
        
    Original methods of PCA class
    ----------
    Apart from the fit method, the following methods of the original PCA class have been modified to augment the incoming X matrix before executing the original function.
    
    -- fit_transform(X)
    -- score(X)
    -- score_samples(X)
    -- transform(X)

    
    Usage Example
    --------
    &gt;&gt;&gt; import numpy as np
    &gt;&gt;&gt; from PDStoolkit import PDS_DPCA
    &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler
    
    &gt;&gt;&gt; X = np.random.rand(30,5)
    &gt;&gt;&gt; scaler = StandardScaler()
    &gt;&gt;&gt; X_scaled = scaler.fit_transform(X)
    
    &gt;&gt;&gt; dpca = PDS_DPCA(n_lags=2)
    &gt;&gt;&gt; dpca.fit(X_scaled, autoFindNLatents=True)
    
    &gt;&gt;&gt; metrics_train = dpca.computeMetrics(X_scaled, isTrainingData=True)
    &gt;&gt;&gt; T2_CL, SPE_CL = dpca.computeThresholds(method=&#39;percentile&#39;)
    &gt;&gt;&gt; dpca.draw_monitoring_charts(metrics=metrics_train, title=&#39;training data&#39;)
    
    &gt;&gt;&gt; X_test = scaler.transform(np.random.rand(30,5))
    &gt;&gt;&gt; abnormalityFlags = dpca.detect_abnormalities(X_test, title=&#39;Test data&#39;)
    &#39;&#39;&#39;
    
    def __init__(self, n_lags=1, **kws):
        
        # sanity check: positive lag
        if n_lags &lt; 1 or (not isinstance(n_lags, int)):
            raise ValueError(&#34;Parameters must be positive integers&#34;)
            
        super().__init__(**kws)
        self.eig_vals_all = None
        self.n_lags = n_lags
    
    def fit(self, X, autoFindNLatents=False, varianceThreshold=0.9):
        &#34;&#34;&#34;
        Extends the fit method of Sklearn PCA to allow in-built augmentation of lagged observations and computation of the &#39;optimal&#39; number of latents.
        
        Parameters:
        ---------------------
        X: ndarray of shape (n_samples, n_features)
            Training data, where n_samples is the number of samples and n_features is the number of features.
            
        autoFindNLatents: bool, optional (default False)
            Bool to indicate whether the number of latents is to be determined by the fit method. 
            If True, number of latents that capture atleast varianceThreshold fraction of the total variance is chosen
            
        varianceThreshold: float, optional (default 0.9)
            Real value between 0 and 1.
            The number of PCs retained is such that fraction of variance captured is atleast varianceThreshold  
        
        Returns
        ---------------------
        self: object
            fitted model
        &#34;&#34;&#34;
        
        # augument data
        X_aug = _augument(X, self.n_lags)
        
        if autoFindNLatents:
            self.n_components = X_aug.shape[1] 
            PCA.fit(self, X_aug)
            self.eig_vals_all = self.explained_variance_ # to be used later for threshold calculations 
            
            # decide # of PCs to retain
            explained_variance = 100*self.explained_variance_ratio_ # in percentage
            cum_explained_variance = np.cumsum(explained_variance) # cumulative % variance explained
            selected_Nlatent = np.argmax(cum_explained_variance &gt;= varianceThreshold*100) + 1
            
            # print message for selected_Nlatent
            print(&#39;# of latents selected: &#39;, selected_Nlatent)
            
            # plot cumulative % variance curve
            plt.figure()
            plt.plot(range(1,X_aug.shape[1]+1), cum_explained_variance, &#39;r+&#39;, label = &#39;cumulative % variance explained&#39;)
            plt.plot(range(1,X_aug.shape[1]+1), explained_variance, &#39;b+&#39; , label = &#39;% variance explained by each PC&#39;)
            plt.ylabel(&#39;Explained variance (in %)&#39;), plt.xlabel(&#39;Principal component number&#39;)
            plt.title(&#39;Variance explained vs # of PCs&#39;), plt.legend()
            plt.show()
            
            # store the computed optimal_Nlatents
            self.n_components = selected_Nlatent
        
        # call the original fit method of PCA class to fit with n_components components
        PCA.fit(self, X_aug)
        
        # ensure eig_vals_all attribute is assigned
        if not autoFindNLatents:
            if self.n_components == X_aug.shape[1]:
                self.eig_vals_all = self.explained_variance_
            else:
                PCA_temp = PCA().fit(X_aug)
                self.eig_vals_all = PCA_temp.explained_variance_
            
        return self
        
    def transform(self, X):
        &#34;&#34;&#34;
        Apply augmentation to X and then perform dimensionality reduction.
        Augmented X is projected on the first principal components previously extracted from a training set.

        Parameters
        ----------
        X: ndarray of shape (n_samples, n_features)
            Data, where n_samples is the number of samples and n_features is the number of features.

        Returns
        -------
        X_transformed : ndarray of shape (n_samples-n_lags, n_components)
            Transformed values.
        &#34;&#34;&#34;
        
        # augument data and call original function
        X_aug = _augument(X, self.n_lags)
        return PCA.transform(self, X_aug)
    
    def fit_transform(self, X):
        &#34;&#34;&#34;
        Augment X, fit the model with augmented X, and apply the dimensionality reduction on augmented X.

        Parameters
        ----------
        X: ndarray of shape (n_samples, n_features)
            Training data, where n_samples is the number of samples and n_features is the number of features.

        Returns
        -------
        X_transformed : ndarray of shape (n_samples-n_lags, n_components)
            Transformed values.
        &#34;&#34;&#34;
        
        # augument data and call original function
        X_aug = _augument(X, self.n_lags)
        return PCA.fit_transform(self, X_aug)
    
    def score(self, X):
        &#34;&#34;&#34;
        Apply augmentation to X and then compute Score. Returns the average log-likelihood of all samples.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            The data.

        Returns
        -------
        ll : float
            Average log-likelihood of the samples under the current model.
        &#34;&#34;&#34;
                
        # augument data and call original function
        X_aug = _augument(X, self.n_lags)
        return PCA.score(self, X_aug)
    
    def score_samples(self, X):
        &#34;&#34;&#34;
        Apply augmentation to X and then compute score for each augmented sample. Returns the log-likelihood of each sample.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data.

        Returns
        -------
        ll : ndarray of shape (n_samples,)
            The first n_lags entries are nan.
            Log-likelihood of each augmented sample under the current model.
        &#34;&#34;&#34;
        
        # augument data and call original function
        X_aug = _augument(X, self.n_lags)
        return np.append(np.repeat(np.nan, self.n_lags), PCA.score_samples(self, X_aug))
    
    def inverse_transform(self, X):
        &#34;&#34;&#34;
        Transform data back to its original augmented space.
        
        Parameters
        ----------
        X: ndarray of shape (n_samples, n_components)
            DPCS score data, where n_samples is the number of samples and n_components is the number of components retained in the DPCA nodel.

        Returns
        -------
        X_augmented_reconstructed : ndarray of shape (n_samples, n_features)
            Reconstructed augmented values.
        &#34;&#34;&#34;
               
        return PCA.inverse_transform(self, X)
                  
    def computeMetrics(self, X, isTrainingData=False):
        &#34;&#34;&#34;
        computes the monitoring indices for the supplied data
        
        Parameters:
        ---------------------
        X: ndarray of shape (n_samples, n_features)
            n_samples is the number of samples and n_features is the number of features.
            
        isTrainingData: bool, optional (default False)
            If True, then the computed metrics are stored as object properties which are utilized during computation of metric thresholds
                          
        Returns:
        ---------------------
        SPE: ndarray of shape (n_samples, ). Also called Q metrics.
            The first n_lags entries are nan.
                        
        T2: ndarray of shape (n_samples, ). Also called Hotelling&#39;s T-square.
            The first n_lags entries are nan.
        &#34;&#34;&#34;
        
        # augument data
        X_aug = _augument(X, self.n_lags)
        
        # compute SPE
        X_aug_scores = self.transform(X)
        X_aug_reconstruct = self.inverse_transform(X_aug_scores) 
        X_aug_error = X_aug - X_aug_reconstruct
        SPE = np.append(np.repeat(np.nan, self.n_lags), np.sum(X_aug_error*X_aug_error, axis = 1))
        
        # compute T2
        if isTrainingData:
            T_cov = np.cov(X_aug_scores.T) # equivalent to np.diag(pca.explained_variance_)
            T_cov_inv = np.linalg.inv(T_cov)
            self.T_cov_inv = T_cov_inv
            
        T2 = np.zeros((X_aug.shape[0],))
        for i in range(X_aug.shape[0]):
            T2[i] = np.dot(np.dot(X_aug_scores[i,:],self.T_cov_inv),X_aug_scores[i,:].T)
        T2 = np.append(np.repeat(np.nan, self.n_lags), T2)
            
        # save metrics for training data as attributes
        if isTrainingData:
            self.T2_train = T2
            self.SPE_train = SPE
        
        return T2, SPE
    
    def computeThresholds(self, method=&#39;percentile&#39;, percentile=99, alpha=0.01):
        &#34;&#34;&#34;
        computes the thresholds / control limits for the monitoring indices from training data
        
        Parameters:
        ---------------------
        method: &#39;percentile&#39; or &#39;statistical&#39;; default &#39;percentile&#39;
            
        percentile: int, optional (default 99)
            The percentile value to use if method=&#39;percentile&#39;
            
        alpha: int, optional (default 99)
            The significance level to use if method=&#39;statistical&#39;. 
            Default value of 0.01 imples 99% control limit.
                        
        Returns
        ---------------------
        T2_CL: float
            Control limit/threshold for T2 metric
        
        SPE_CL: float
            Control limit/threshold for SPE metric
        &#34;&#34;&#34;
        
        if not hasattr(self, &#39;T2_train&#39;):
            raise AttributeError(&#39;Training metrices not found. Run computeMetrics method before computing the thresholds.&#39;)
            
        if method == &#39;percentile&#39;:
            T2_CL = np.nanpercentile(self.T2_train, percentile)
            SPE_CL = np.nanpercentile(self.SPE_train, percentile)
                       
        elif method == &#39;statistical&#39;:
            # parameters
            N = self.T2_train.shape[0]
            k = self.n_components
            m = len(self.eig_vals_all)
            
            # T2_CL
            T2_CL = k*(N**2-1)*scipy.stats.f.ppf(1-alpha,k,N-k)/(N*(N-k))

            # SPE_CL
            theta1 = np.sum(self.eig_vals_all[k:])
            theta2 = np.sum([self.eig_vals_all[j]**2 for j in range(k,m)])
            theta3 = np.sum([self.eig_vals_all[j]**3 for j in range(k,m)])
            if theta3 == 0:
                raise ValueError(&#39;Division by zero encountered. Statistical threshold computation for SPE_CL is not possible&#39;)
            h0 = 1-2*theta1*theta3/(3*theta2**2)
            z_alpha = scipy.stats.norm.ppf(1-alpha)
            SPE_CL = theta1*(z_alpha*np.sqrt(2*theta2*h0**2)/theta1+ 1 + theta2*h0*(1-h0)/theta1**2)**2 
        
        else:
            raise ValueError(&#39;Incorrect choice of method parameter&#39;)
            
        # save control limits as attributes
        self.T2_CL = T2_CL
        self.SPE_CL = SPE_CL
                
        return T2_CL, SPE_CL
    
    def draw_monitoring_charts(self, metrics=None, logScaleY=False, title=&#39;&#39;):
        &#34;&#34;&#34; 
        Draw the monitoring charts for the training or test data.
        The control limits are plotted as red dashed line. 
        
        Parameters:
        ---------------------
        metrics: list or tuple of monitoring metrics (1D numpy arrays). Should follow the order (T2, SPE)
            If not specified, then the object&#39;s stored metrics from training data are used.
                
        title: str, optional 
            Title for the charts                      
        &#34;&#34;&#34;
        
        if metrics is None:
            metrics = (self.T2_train, self.SPE_train)
        
        # T2
        plt.figure()
        plt.plot(metrics[0], color=&#39;k&#39;, linestyle = &#39;:&#39;, marker=&#39;o&#39;, markerfacecolor = &#39;C4&#39;)
        if hasattr(self, &#39;T2_CL&#39;):
            plt.axhline(self.T2_CL, color = &#34;red&#34;, linestyle = &#34;-.&#34;)
        plt.xlabel(&#39;Sample #&#39;), plt.ylabel(&#39;T2&#39;)
        if logScaleY:
            plt.yscale(&#39;log&#39;)
        plt.title(title), plt.show()
        
        # SPE
        plt.figure()
        plt.plot(metrics[1], color=&#39;k&#39;, linestyle = &#39;:&#39;, marker=&#39;o&#39;, markerfacecolor = &#39;C4&#39;)
        if hasattr(self, &#39;SPE_CL&#39;):
            plt.axhline(self.SPE_CL, color = &#34;red&#34;, linestyle = &#34;-.&#34;)
        plt.xlabel(&#39;Sample #&#39;), plt.ylabel(&#39;SPE&#39;)
        if logScaleY:
            plt.yscale(&#39;log&#39;)
        plt.title(title), plt.show()
    
    def fit_4_monitoring(self, X, autoFindNLatents=False, varianceThreshold=0.9, method=&#39;percentile&#39;, percentile=99, alpha=0.01):
        &#34;&#34;&#34;
        A utility method that calls the following model-training related methods in succession.
        
        fit(X, autoFindNLatents, varianceThreshold)
        computeMetrics(X, isTrainingData=True)
        computeThresholds(method, percentile, alpha)
        draw_monitoring_charts(title=&#39;training data&#39;)
        
        Note: Check doc string of individual methods for details on the method parameters.
        &#34;&#34;&#34;
        
        self.fit(X, autoFindNLatents, varianceThreshold)
        self.computeMetrics(X, isTrainingData=True)
        self.computeThresholds(method, percentile, alpha)
        self.draw_monitoring_charts(title=&#39;training data&#39;)
        
        return self
    
    def detect_abnormalities(self, X, drawMonitoringChart=True, title=&#39;&#39;):
        &#34;&#34;&#34;
        Detects if the observations are abnormal or normal. 
        Detection is based on the logic that for a &#39;normal&#39; sample, the monitoring metrics should lie below their thresholds.
        
        Parameters:
        ---------------------
        X: ndarray of shape (n_samples, n_features)
            n_samples is the number of samples and n_features is the number of features.
        
        drawMonitoringChart: bool, optional (default True)
            If True, then the monitoring charts are also drawn.
        
        title: str, optional 
            Title for the charts     
                                          
        Returns:
        ---------------------
        abnormalityFlags: ndarray of shape (n_samples, )
            Returns 1 for abnormal samples and 0 for normal samples.
            The first n_lags entries are nan.
        &#34;&#34;&#34;
        
        T2, SPE = self.computeMetrics(X)
        abnormalityFlags = np.logical_or(T2 &gt; self.T2_CL, SPE &gt; self.SPE_CL)
        print(&#39;Number of abnormal sample(s): &#39;, np.sum(abnormalityFlags))
        
        abnormalityFlags = np.append(np.repeat(np.nan, self.n_lags), abnormalityFlags)        
        if drawMonitoringChart:
            self.draw_monitoring_charts(metrics=(T2, SPE), title=title)
                
        return abnormalityFlags
        
        
        
        </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="PDS_DPCA.PDS_DPCA"><code class="flex name class">
<span>class <span class="ident">PDS_DPCA</span></span>
<span>(</span><span>n_lags=1, **kws)</span>
</code></dt>
<dd>
<div class="desc"><p>This class wraps Sklearn's PCA class to provide the following additional methods</p>
<p>1) computeMetrics: computes the monitoring indices for the supplied data
2) computeThresholds: computes the thresholds / control limits for the monitoring indices from training data
3) draw_monitoring_charts: Draw the monitoring charts for the training or test data
4) detect_abnormalities: Detects if the observations are abnormal or normal samples</p>
<h2 id="additional-parameters">Additional Parameters</h2>
<p>n_lags: integer (default 1)
The number of lags to be used for data augumentation.</p>
<h2 id="additional-attributes">Additional Attributes</h2>
<p>n_lags: integer
The number of lags used for data augumentation.</p>
<h2 id="original-methods-of-pca-class">Original Methods Of Pca Class</h2>
<p>Apart from the fit method, the following methods of the original PCA class have been modified to augment the incoming X matrix before executing the original function.</p>
<p>&ndash; fit_transform(X)
&ndash; score(X)
&ndash; score_samples(X)
&ndash; transform(X)</p>
<h2 id="usage-example">Usage Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from PDStoolkit import PDS_DPCA
&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; X = np.random.rand(30,5)
&gt;&gt;&gt; scaler = StandardScaler()
&gt;&gt;&gt; X_scaled = scaler.fit_transform(X)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; dpca = PDS_DPCA(n_lags=2)
&gt;&gt;&gt; dpca.fit(X_scaled, autoFindNLatents=True)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; metrics_train = dpca.computeMetrics(X_scaled, isTrainingData=True)
&gt;&gt;&gt; T2_CL, SPE_CL = dpca.computeThresholds(method='percentile')
&gt;&gt;&gt; dpca.draw_monitoring_charts(metrics=metrics_train, title='training data')
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; X_test = scaler.transform(np.random.rand(30,5))
&gt;&gt;&gt; abnormalityFlags = dpca.detect_abnormalities(X_test, title='Test data')
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PDS_DPCA(PCA):
    &#39;&#39;&#39; This class wraps Sklearn&#39;s PCA class to provide the following additional methods
    
    1) computeMetrics: computes the monitoring indices for the supplied data
    2) computeThresholds: computes the thresholds / control limits for the monitoring indices from training data
    3) draw_monitoring_charts: Draw the monitoring charts for the training or test data
    4) detect_abnormalities: Detects if the observations are abnormal or normal samples
    
    Additional Parameters
    ----------
    n_lags: integer (default 1)
        The number of lags to be used for data augumentation.
        
    
    Additional Attributes
    ----------
    n_lags: integer
        The number of lags used for data augumentation.
        
        
    Original methods of PCA class
    ----------
    Apart from the fit method, the following methods of the original PCA class have been modified to augment the incoming X matrix before executing the original function.
    
    -- fit_transform(X)
    -- score(X)
    -- score_samples(X)
    -- transform(X)

    
    Usage Example
    --------
    &gt;&gt;&gt; import numpy as np
    &gt;&gt;&gt; from PDStoolkit import PDS_DPCA
    &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler
    
    &gt;&gt;&gt; X = np.random.rand(30,5)
    &gt;&gt;&gt; scaler = StandardScaler()
    &gt;&gt;&gt; X_scaled = scaler.fit_transform(X)
    
    &gt;&gt;&gt; dpca = PDS_DPCA(n_lags=2)
    &gt;&gt;&gt; dpca.fit(X_scaled, autoFindNLatents=True)
    
    &gt;&gt;&gt; metrics_train = dpca.computeMetrics(X_scaled, isTrainingData=True)
    &gt;&gt;&gt; T2_CL, SPE_CL = dpca.computeThresholds(method=&#39;percentile&#39;)
    &gt;&gt;&gt; dpca.draw_monitoring_charts(metrics=metrics_train, title=&#39;training data&#39;)
    
    &gt;&gt;&gt; X_test = scaler.transform(np.random.rand(30,5))
    &gt;&gt;&gt; abnormalityFlags = dpca.detect_abnormalities(X_test, title=&#39;Test data&#39;)
    &#39;&#39;&#39;
    
    def __init__(self, n_lags=1, **kws):
        
        # sanity check: positive lag
        if n_lags &lt; 1 or (not isinstance(n_lags, int)):
            raise ValueError(&#34;Parameters must be positive integers&#34;)
            
        super().__init__(**kws)
        self.eig_vals_all = None
        self.n_lags = n_lags
    
    def fit(self, X, autoFindNLatents=False, varianceThreshold=0.9):
        &#34;&#34;&#34;
        Extends the fit method of Sklearn PCA to allow in-built augmentation of lagged observations and computation of the &#39;optimal&#39; number of latents.
        
        Parameters:
        ---------------------
        X: ndarray of shape (n_samples, n_features)
            Training data, where n_samples is the number of samples and n_features is the number of features.
            
        autoFindNLatents: bool, optional (default False)
            Bool to indicate whether the number of latents is to be determined by the fit method. 
            If True, number of latents that capture atleast varianceThreshold fraction of the total variance is chosen
            
        varianceThreshold: float, optional (default 0.9)
            Real value between 0 and 1.
            The number of PCs retained is such that fraction of variance captured is atleast varianceThreshold  
        
        Returns
        ---------------------
        self: object
            fitted model
        &#34;&#34;&#34;
        
        # augument data
        X_aug = _augument(X, self.n_lags)
        
        if autoFindNLatents:
            self.n_components = X_aug.shape[1] 
            PCA.fit(self, X_aug)
            self.eig_vals_all = self.explained_variance_ # to be used later for threshold calculations 
            
            # decide # of PCs to retain
            explained_variance = 100*self.explained_variance_ratio_ # in percentage
            cum_explained_variance = np.cumsum(explained_variance) # cumulative % variance explained
            selected_Nlatent = np.argmax(cum_explained_variance &gt;= varianceThreshold*100) + 1
            
            # print message for selected_Nlatent
            print(&#39;# of latents selected: &#39;, selected_Nlatent)
            
            # plot cumulative % variance curve
            plt.figure()
            plt.plot(range(1,X_aug.shape[1]+1), cum_explained_variance, &#39;r+&#39;, label = &#39;cumulative % variance explained&#39;)
            plt.plot(range(1,X_aug.shape[1]+1), explained_variance, &#39;b+&#39; , label = &#39;% variance explained by each PC&#39;)
            plt.ylabel(&#39;Explained variance (in %)&#39;), plt.xlabel(&#39;Principal component number&#39;)
            plt.title(&#39;Variance explained vs # of PCs&#39;), plt.legend()
            plt.show()
            
            # store the computed optimal_Nlatents
            self.n_components = selected_Nlatent
        
        # call the original fit method of PCA class to fit with n_components components
        PCA.fit(self, X_aug)
        
        # ensure eig_vals_all attribute is assigned
        if not autoFindNLatents:
            if self.n_components == X_aug.shape[1]:
                self.eig_vals_all = self.explained_variance_
            else:
                PCA_temp = PCA().fit(X_aug)
                self.eig_vals_all = PCA_temp.explained_variance_
            
        return self
        
    def transform(self, X):
        &#34;&#34;&#34;
        Apply augmentation to X and then perform dimensionality reduction.
        Augmented X is projected on the first principal components previously extracted from a training set.

        Parameters
        ----------
        X: ndarray of shape (n_samples, n_features)
            Data, where n_samples is the number of samples and n_features is the number of features.

        Returns
        -------
        X_transformed : ndarray of shape (n_samples-n_lags, n_components)
            Transformed values.
        &#34;&#34;&#34;
        
        # augument data and call original function
        X_aug = _augument(X, self.n_lags)
        return PCA.transform(self, X_aug)
    
    def fit_transform(self, X):
        &#34;&#34;&#34;
        Augment X, fit the model with augmented X, and apply the dimensionality reduction on augmented X.

        Parameters
        ----------
        X: ndarray of shape (n_samples, n_features)
            Training data, where n_samples is the number of samples and n_features is the number of features.

        Returns
        -------
        X_transformed : ndarray of shape (n_samples-n_lags, n_components)
            Transformed values.
        &#34;&#34;&#34;
        
        # augument data and call original function
        X_aug = _augument(X, self.n_lags)
        return PCA.fit_transform(self, X_aug)
    
    def score(self, X):
        &#34;&#34;&#34;
        Apply augmentation to X and then compute Score. Returns the average log-likelihood of all samples.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            The data.

        Returns
        -------
        ll : float
            Average log-likelihood of the samples under the current model.
        &#34;&#34;&#34;
                
        # augument data and call original function
        X_aug = _augument(X, self.n_lags)
        return PCA.score(self, X_aug)
    
    def score_samples(self, X):
        &#34;&#34;&#34;
        Apply augmentation to X and then compute score for each augmented sample. Returns the log-likelihood of each sample.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data.

        Returns
        -------
        ll : ndarray of shape (n_samples,)
            The first n_lags entries are nan.
            Log-likelihood of each augmented sample under the current model.
        &#34;&#34;&#34;
        
        # augument data and call original function
        X_aug = _augument(X, self.n_lags)
        return np.append(np.repeat(np.nan, self.n_lags), PCA.score_samples(self, X_aug))
    
    def inverse_transform(self, X):
        &#34;&#34;&#34;
        Transform data back to its original augmented space.
        
        Parameters
        ----------
        X: ndarray of shape (n_samples, n_components)
            DPCS score data, where n_samples is the number of samples and n_components is the number of components retained in the DPCA nodel.

        Returns
        -------
        X_augmented_reconstructed : ndarray of shape (n_samples, n_features)
            Reconstructed augmented values.
        &#34;&#34;&#34;
               
        return PCA.inverse_transform(self, X)
                  
    def computeMetrics(self, X, isTrainingData=False):
        &#34;&#34;&#34;
        computes the monitoring indices for the supplied data
        
        Parameters:
        ---------------------
        X: ndarray of shape (n_samples, n_features)
            n_samples is the number of samples and n_features is the number of features.
            
        isTrainingData: bool, optional (default False)
            If True, then the computed metrics are stored as object properties which are utilized during computation of metric thresholds
                          
        Returns:
        ---------------------
        SPE: ndarray of shape (n_samples, ). Also called Q metrics.
            The first n_lags entries are nan.
                        
        T2: ndarray of shape (n_samples, ). Also called Hotelling&#39;s T-square.
            The first n_lags entries are nan.
        &#34;&#34;&#34;
        
        # augument data
        X_aug = _augument(X, self.n_lags)
        
        # compute SPE
        X_aug_scores = self.transform(X)
        X_aug_reconstruct = self.inverse_transform(X_aug_scores) 
        X_aug_error = X_aug - X_aug_reconstruct
        SPE = np.append(np.repeat(np.nan, self.n_lags), np.sum(X_aug_error*X_aug_error, axis = 1))
        
        # compute T2
        if isTrainingData:
            T_cov = np.cov(X_aug_scores.T) # equivalent to np.diag(pca.explained_variance_)
            T_cov_inv = np.linalg.inv(T_cov)
            self.T_cov_inv = T_cov_inv
            
        T2 = np.zeros((X_aug.shape[0],))
        for i in range(X_aug.shape[0]):
            T2[i] = np.dot(np.dot(X_aug_scores[i,:],self.T_cov_inv),X_aug_scores[i,:].T)
        T2 = np.append(np.repeat(np.nan, self.n_lags), T2)
            
        # save metrics for training data as attributes
        if isTrainingData:
            self.T2_train = T2
            self.SPE_train = SPE
        
        return T2, SPE
    
    def computeThresholds(self, method=&#39;percentile&#39;, percentile=99, alpha=0.01):
        &#34;&#34;&#34;
        computes the thresholds / control limits for the monitoring indices from training data
        
        Parameters:
        ---------------------
        method: &#39;percentile&#39; or &#39;statistical&#39;; default &#39;percentile&#39;
            
        percentile: int, optional (default 99)
            The percentile value to use if method=&#39;percentile&#39;
            
        alpha: int, optional (default 99)
            The significance level to use if method=&#39;statistical&#39;. 
            Default value of 0.01 imples 99% control limit.
                        
        Returns
        ---------------------
        T2_CL: float
            Control limit/threshold for T2 metric
        
        SPE_CL: float
            Control limit/threshold for SPE metric
        &#34;&#34;&#34;
        
        if not hasattr(self, &#39;T2_train&#39;):
            raise AttributeError(&#39;Training metrices not found. Run computeMetrics method before computing the thresholds.&#39;)
            
        if method == &#39;percentile&#39;:
            T2_CL = np.nanpercentile(self.T2_train, percentile)
            SPE_CL = np.nanpercentile(self.SPE_train, percentile)
                       
        elif method == &#39;statistical&#39;:
            # parameters
            N = self.T2_train.shape[0]
            k = self.n_components
            m = len(self.eig_vals_all)
            
            # T2_CL
            T2_CL = k*(N**2-1)*scipy.stats.f.ppf(1-alpha,k,N-k)/(N*(N-k))

            # SPE_CL
            theta1 = np.sum(self.eig_vals_all[k:])
            theta2 = np.sum([self.eig_vals_all[j]**2 for j in range(k,m)])
            theta3 = np.sum([self.eig_vals_all[j]**3 for j in range(k,m)])
            if theta3 == 0:
                raise ValueError(&#39;Division by zero encountered. Statistical threshold computation for SPE_CL is not possible&#39;)
            h0 = 1-2*theta1*theta3/(3*theta2**2)
            z_alpha = scipy.stats.norm.ppf(1-alpha)
            SPE_CL = theta1*(z_alpha*np.sqrt(2*theta2*h0**2)/theta1+ 1 + theta2*h0*(1-h0)/theta1**2)**2 
        
        else:
            raise ValueError(&#39;Incorrect choice of method parameter&#39;)
            
        # save control limits as attributes
        self.T2_CL = T2_CL
        self.SPE_CL = SPE_CL
                
        return T2_CL, SPE_CL
    
    def draw_monitoring_charts(self, metrics=None, logScaleY=False, title=&#39;&#39;):
        &#34;&#34;&#34; 
        Draw the monitoring charts for the training or test data.
        The control limits are plotted as red dashed line. 
        
        Parameters:
        ---------------------
        metrics: list or tuple of monitoring metrics (1D numpy arrays). Should follow the order (T2, SPE)
            If not specified, then the object&#39;s stored metrics from training data are used.
                
        title: str, optional 
            Title for the charts                      
        &#34;&#34;&#34;
        
        if metrics is None:
            metrics = (self.T2_train, self.SPE_train)
        
        # T2
        plt.figure()
        plt.plot(metrics[0], color=&#39;k&#39;, linestyle = &#39;:&#39;, marker=&#39;o&#39;, markerfacecolor = &#39;C4&#39;)
        if hasattr(self, &#39;T2_CL&#39;):
            plt.axhline(self.T2_CL, color = &#34;red&#34;, linestyle = &#34;-.&#34;)
        plt.xlabel(&#39;Sample #&#39;), plt.ylabel(&#39;T2&#39;)
        if logScaleY:
            plt.yscale(&#39;log&#39;)
        plt.title(title), plt.show()
        
        # SPE
        plt.figure()
        plt.plot(metrics[1], color=&#39;k&#39;, linestyle = &#39;:&#39;, marker=&#39;o&#39;, markerfacecolor = &#39;C4&#39;)
        if hasattr(self, &#39;SPE_CL&#39;):
            plt.axhline(self.SPE_CL, color = &#34;red&#34;, linestyle = &#34;-.&#34;)
        plt.xlabel(&#39;Sample #&#39;), plt.ylabel(&#39;SPE&#39;)
        if logScaleY:
            plt.yscale(&#39;log&#39;)
        plt.title(title), plt.show()
    
    def fit_4_monitoring(self, X, autoFindNLatents=False, varianceThreshold=0.9, method=&#39;percentile&#39;, percentile=99, alpha=0.01):
        &#34;&#34;&#34;
        A utility method that calls the following model-training related methods in succession.
        
        fit(X, autoFindNLatents, varianceThreshold)
        computeMetrics(X, isTrainingData=True)
        computeThresholds(method, percentile, alpha)
        draw_monitoring_charts(title=&#39;training data&#39;)
        
        Note: Check doc string of individual methods for details on the method parameters.
        &#34;&#34;&#34;
        
        self.fit(X, autoFindNLatents, varianceThreshold)
        self.computeMetrics(X, isTrainingData=True)
        self.computeThresholds(method, percentile, alpha)
        self.draw_monitoring_charts(title=&#39;training data&#39;)
        
        return self
    
    def detect_abnormalities(self, X, drawMonitoringChart=True, title=&#39;&#39;):
        &#34;&#34;&#34;
        Detects if the observations are abnormal or normal. 
        Detection is based on the logic that for a &#39;normal&#39; sample, the monitoring metrics should lie below their thresholds.
        
        Parameters:
        ---------------------
        X: ndarray of shape (n_samples, n_features)
            n_samples is the number of samples and n_features is the number of features.
        
        drawMonitoringChart: bool, optional (default True)
            If True, then the monitoring charts are also drawn.
        
        title: str, optional 
            Title for the charts     
                                          
        Returns:
        ---------------------
        abnormalityFlags: ndarray of shape (n_samples, )
            Returns 1 for abnormal samples and 0 for normal samples.
            The first n_lags entries are nan.
        &#34;&#34;&#34;
        
        T2, SPE = self.computeMetrics(X)
        abnormalityFlags = np.logical_or(T2 &gt; self.T2_CL, SPE &gt; self.SPE_CL)
        print(&#39;Number of abnormal sample(s): &#39;, np.sum(abnormalityFlags))
        
        abnormalityFlags = np.append(np.repeat(np.nan, self.n_lags), abnormalityFlags)        
        if drawMonitoringChart:
            self.draw_monitoring_charts(metrics=(T2, SPE), title=title)
                
        return abnormalityFlags</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.decomposition._pca.PCA</li>
<li>sklearn.decomposition._base._BasePCA</li>
<li>sklearn.base.TransformerMixin</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PDS_DPCA.PDS_DPCA.computeMetrics"><code class="name flex">
<span>def <span class="ident">computeMetrics</span></span>(<span>self, X, isTrainingData=False)</span>
</code></dt>
<dd>
<div class="desc"><p>computes the monitoring indices for the supplied data</p>
<h2 id="parameters">Parameters:</h2>
<p>X: ndarray of shape (n_samples, n_features)
n_samples is the number of samples and n_features is the number of features.</p>
<p>isTrainingData: bool, optional (default False)
If True, then the computed metrics are stored as object properties which are utilized during computation of metric thresholds</p>
<h2 id="returns">Returns:</h2>
<p>SPE: ndarray of shape (n_samples, ). Also called Q metrics.
The first n_lags entries are nan.</p>
<p>T2: ndarray of shape (n_samples, ). Also called Hotelling's T-square.
The first n_lags entries are nan.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def computeMetrics(self, X, isTrainingData=False):
    &#34;&#34;&#34;
    computes the monitoring indices for the supplied data
    
    Parameters:
    ---------------------
    X: ndarray of shape (n_samples, n_features)
        n_samples is the number of samples and n_features is the number of features.
        
    isTrainingData: bool, optional (default False)
        If True, then the computed metrics are stored as object properties which are utilized during computation of metric thresholds
                      
    Returns:
    ---------------------
    SPE: ndarray of shape (n_samples, ). Also called Q metrics.
        The first n_lags entries are nan.
                    
    T2: ndarray of shape (n_samples, ). Also called Hotelling&#39;s T-square.
        The first n_lags entries are nan.
    &#34;&#34;&#34;
    
    # augument data
    X_aug = _augument(X, self.n_lags)
    
    # compute SPE
    X_aug_scores = self.transform(X)
    X_aug_reconstruct = self.inverse_transform(X_aug_scores) 
    X_aug_error = X_aug - X_aug_reconstruct
    SPE = np.append(np.repeat(np.nan, self.n_lags), np.sum(X_aug_error*X_aug_error, axis = 1))
    
    # compute T2
    if isTrainingData:
        T_cov = np.cov(X_aug_scores.T) # equivalent to np.diag(pca.explained_variance_)
        T_cov_inv = np.linalg.inv(T_cov)
        self.T_cov_inv = T_cov_inv
        
    T2 = np.zeros((X_aug.shape[0],))
    for i in range(X_aug.shape[0]):
        T2[i] = np.dot(np.dot(X_aug_scores[i,:],self.T_cov_inv),X_aug_scores[i,:].T)
    T2 = np.append(np.repeat(np.nan, self.n_lags), T2)
        
    # save metrics for training data as attributes
    if isTrainingData:
        self.T2_train = T2
        self.SPE_train = SPE
    
    return T2, SPE</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.computeThresholds"><code class="name flex">
<span>def <span class="ident">computeThresholds</span></span>(<span>self, method='percentile', percentile=99, alpha=0.01)</span>
</code></dt>
<dd>
<div class="desc"><p>computes the thresholds / control limits for the monitoring indices from training data</p>
<h2 id="parameters">Parameters:</h2>
<p>method: 'percentile' or 'statistical'; default 'percentile'</p>
<p>percentile: int, optional (default 99)
The percentile value to use if method='percentile'</p>
<p>alpha: int, optional (default 99)
The significance level to use if method='statistical'.
Default value of 0.01 imples 99% control limit.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>T2_CL</code></strong> :&ensp;<code>float</code></dt>
<dd>Control limit/threshold for T2 metric</dd>
<dt><strong><code>SPE_CL</code></strong> :&ensp;<code>float</code></dt>
<dd>Control limit/threshold for SPE metric</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def computeThresholds(self, method=&#39;percentile&#39;, percentile=99, alpha=0.01):
    &#34;&#34;&#34;
    computes the thresholds / control limits for the monitoring indices from training data
    
    Parameters:
    ---------------------
    method: &#39;percentile&#39; or &#39;statistical&#39;; default &#39;percentile&#39;
        
    percentile: int, optional (default 99)
        The percentile value to use if method=&#39;percentile&#39;
        
    alpha: int, optional (default 99)
        The significance level to use if method=&#39;statistical&#39;. 
        Default value of 0.01 imples 99% control limit.
                    
    Returns
    ---------------------
    T2_CL: float
        Control limit/threshold for T2 metric
    
    SPE_CL: float
        Control limit/threshold for SPE metric
    &#34;&#34;&#34;
    
    if not hasattr(self, &#39;T2_train&#39;):
        raise AttributeError(&#39;Training metrices not found. Run computeMetrics method before computing the thresholds.&#39;)
        
    if method == &#39;percentile&#39;:
        T2_CL = np.nanpercentile(self.T2_train, percentile)
        SPE_CL = np.nanpercentile(self.SPE_train, percentile)
                   
    elif method == &#39;statistical&#39;:
        # parameters
        N = self.T2_train.shape[0]
        k = self.n_components
        m = len(self.eig_vals_all)
        
        # T2_CL
        T2_CL = k*(N**2-1)*scipy.stats.f.ppf(1-alpha,k,N-k)/(N*(N-k))

        # SPE_CL
        theta1 = np.sum(self.eig_vals_all[k:])
        theta2 = np.sum([self.eig_vals_all[j]**2 for j in range(k,m)])
        theta3 = np.sum([self.eig_vals_all[j]**3 for j in range(k,m)])
        if theta3 == 0:
            raise ValueError(&#39;Division by zero encountered. Statistical threshold computation for SPE_CL is not possible&#39;)
        h0 = 1-2*theta1*theta3/(3*theta2**2)
        z_alpha = scipy.stats.norm.ppf(1-alpha)
        SPE_CL = theta1*(z_alpha*np.sqrt(2*theta2*h0**2)/theta1+ 1 + theta2*h0*(1-h0)/theta1**2)**2 
    
    else:
        raise ValueError(&#39;Incorrect choice of method parameter&#39;)
        
    # save control limits as attributes
    self.T2_CL = T2_CL
    self.SPE_CL = SPE_CL
            
    return T2_CL, SPE_CL</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.detect_abnormalities"><code class="name flex">
<span>def <span class="ident">detect_abnormalities</span></span>(<span>self, X, drawMonitoringChart=True, title='')</span>
</code></dt>
<dd>
<div class="desc"><p>Detects if the observations are abnormal or normal.
Detection is based on the logic that for a 'normal' sample, the monitoring metrics should lie below their thresholds.</p>
<h2 id="parameters">Parameters:</h2>
<p>X: ndarray of shape (n_samples, n_features)
n_samples is the number of samples and n_features is the number of features.</p>
<p>drawMonitoringChart: bool, optional (default True)
If True, then the monitoring charts are also drawn.</p>
<p>title: str, optional
Title for the charts
</p>
<h2 id="returns">Returns:</h2>
<p>abnormalityFlags: ndarray of shape (n_samples, )
Returns 1 for abnormal samples and 0 for normal samples.
The first n_lags entries are nan.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect_abnormalities(self, X, drawMonitoringChart=True, title=&#39;&#39;):
    &#34;&#34;&#34;
    Detects if the observations are abnormal or normal. 
    Detection is based on the logic that for a &#39;normal&#39; sample, the monitoring metrics should lie below their thresholds.
    
    Parameters:
    ---------------------
    X: ndarray of shape (n_samples, n_features)
        n_samples is the number of samples and n_features is the number of features.
    
    drawMonitoringChart: bool, optional (default True)
        If True, then the monitoring charts are also drawn.
    
    title: str, optional 
        Title for the charts     
                                      
    Returns:
    ---------------------
    abnormalityFlags: ndarray of shape (n_samples, )
        Returns 1 for abnormal samples and 0 for normal samples.
        The first n_lags entries are nan.
    &#34;&#34;&#34;
    
    T2, SPE = self.computeMetrics(X)
    abnormalityFlags = np.logical_or(T2 &gt; self.T2_CL, SPE &gt; self.SPE_CL)
    print(&#39;Number of abnormal sample(s): &#39;, np.sum(abnormalityFlags))
    
    abnormalityFlags = np.append(np.repeat(np.nan, self.n_lags), abnormalityFlags)        
    if drawMonitoringChart:
        self.draw_monitoring_charts(metrics=(T2, SPE), title=title)
            
    return abnormalityFlags</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.draw_monitoring_charts"><code class="name flex">
<span>def <span class="ident">draw_monitoring_charts</span></span>(<span>self, metrics=None, logScaleY=False, title='')</span>
</code></dt>
<dd>
<div class="desc"><p>Draw the monitoring charts for the training or test data.
The control limits are plotted as red dashed line. </p>
<h2 id="parameters">Parameters:</h2>
<p>metrics: list or tuple of monitoring metrics (1D numpy arrays). Should follow the order (T2, SPE)
If not specified, then the object's stored metrics from training data are used.</p>
<p>title: str, optional
Title for the charts</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def draw_monitoring_charts(self, metrics=None, logScaleY=False, title=&#39;&#39;):
    &#34;&#34;&#34; 
    Draw the monitoring charts for the training or test data.
    The control limits are plotted as red dashed line. 
    
    Parameters:
    ---------------------
    metrics: list or tuple of monitoring metrics (1D numpy arrays). Should follow the order (T2, SPE)
        If not specified, then the object&#39;s stored metrics from training data are used.
            
    title: str, optional 
        Title for the charts                      
    &#34;&#34;&#34;
    
    if metrics is None:
        metrics = (self.T2_train, self.SPE_train)
    
    # T2
    plt.figure()
    plt.plot(metrics[0], color=&#39;k&#39;, linestyle = &#39;:&#39;, marker=&#39;o&#39;, markerfacecolor = &#39;C4&#39;)
    if hasattr(self, &#39;T2_CL&#39;):
        plt.axhline(self.T2_CL, color = &#34;red&#34;, linestyle = &#34;-.&#34;)
    plt.xlabel(&#39;Sample #&#39;), plt.ylabel(&#39;T2&#39;)
    if logScaleY:
        plt.yscale(&#39;log&#39;)
    plt.title(title), plt.show()
    
    # SPE
    plt.figure()
    plt.plot(metrics[1], color=&#39;k&#39;, linestyle = &#39;:&#39;, marker=&#39;o&#39;, markerfacecolor = &#39;C4&#39;)
    if hasattr(self, &#39;SPE_CL&#39;):
        plt.axhline(self.SPE_CL, color = &#34;red&#34;, linestyle = &#34;-.&#34;)
    plt.xlabel(&#39;Sample #&#39;), plt.ylabel(&#39;SPE&#39;)
    if logScaleY:
        plt.yscale(&#39;log&#39;)
    plt.title(title), plt.show()</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, autoFindNLatents=False, varianceThreshold=0.9)</span>
</code></dt>
<dd>
<div class="desc"><p>Extends the fit method of Sklearn PCA to allow in-built augmentation of lagged observations and computation of the 'optimal' number of latents.</p>
<h2 id="parameters">Parameters:</h2>
<p>X: ndarray of shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number of features.</p>
<p>autoFindNLatents: bool, optional (default False)
Bool to indicate whether the number of latents is to be determined by the fit method.
If True, number of latents that capture atleast varianceThreshold fraction of the total variance is chosen</p>
<p>varianceThreshold: float, optional (default 0.9)
Real value between 0 and 1.
The number of PCs retained is such that fraction of variance captured is atleast varianceThreshold
</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>fitted model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, autoFindNLatents=False, varianceThreshold=0.9):
    &#34;&#34;&#34;
    Extends the fit method of Sklearn PCA to allow in-built augmentation of lagged observations and computation of the &#39;optimal&#39; number of latents.
    
    Parameters:
    ---------------------
    X: ndarray of shape (n_samples, n_features)
        Training data, where n_samples is the number of samples and n_features is the number of features.
        
    autoFindNLatents: bool, optional (default False)
        Bool to indicate whether the number of latents is to be determined by the fit method. 
        If True, number of latents that capture atleast varianceThreshold fraction of the total variance is chosen
        
    varianceThreshold: float, optional (default 0.9)
        Real value between 0 and 1.
        The number of PCs retained is such that fraction of variance captured is atleast varianceThreshold  
    
    Returns
    ---------------------
    self: object
        fitted model
    &#34;&#34;&#34;
    
    # augument data
    X_aug = _augument(X, self.n_lags)
    
    if autoFindNLatents:
        self.n_components = X_aug.shape[1] 
        PCA.fit(self, X_aug)
        self.eig_vals_all = self.explained_variance_ # to be used later for threshold calculations 
        
        # decide # of PCs to retain
        explained_variance = 100*self.explained_variance_ratio_ # in percentage
        cum_explained_variance = np.cumsum(explained_variance) # cumulative % variance explained
        selected_Nlatent = np.argmax(cum_explained_variance &gt;= varianceThreshold*100) + 1
        
        # print message for selected_Nlatent
        print(&#39;# of latents selected: &#39;, selected_Nlatent)
        
        # plot cumulative % variance curve
        plt.figure()
        plt.plot(range(1,X_aug.shape[1]+1), cum_explained_variance, &#39;r+&#39;, label = &#39;cumulative % variance explained&#39;)
        plt.plot(range(1,X_aug.shape[1]+1), explained_variance, &#39;b+&#39; , label = &#39;% variance explained by each PC&#39;)
        plt.ylabel(&#39;Explained variance (in %)&#39;), plt.xlabel(&#39;Principal component number&#39;)
        plt.title(&#39;Variance explained vs # of PCs&#39;), plt.legend()
        plt.show()
        
        # store the computed optimal_Nlatents
        self.n_components = selected_Nlatent
    
    # call the original fit method of PCA class to fit with n_components components
    PCA.fit(self, X_aug)
    
    # ensure eig_vals_all attribute is assigned
    if not autoFindNLatents:
        if self.n_components == X_aug.shape[1]:
            self.eig_vals_all = self.explained_variance_
        else:
            PCA_temp = PCA().fit(X_aug)
            self.eig_vals_all = PCA_temp.explained_variance_
        
    return self</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.fit_4_monitoring"><code class="name flex">
<span>def <span class="ident">fit_4_monitoring</span></span>(<span>self, X, autoFindNLatents=False, varianceThreshold=0.9, method='percentile', percentile=99, alpha=0.01)</span>
</code></dt>
<dd>
<div class="desc"><p>A utility method that calls the following model-training related methods in succession.</p>
<p>fit(X, autoFindNLatents, varianceThreshold)
computeMetrics(X, isTrainingData=True)
computeThresholds(method, percentile, alpha)
draw_monitoring_charts(title='training data')</p>
<p>Note: Check doc string of individual methods for details on the method parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_4_monitoring(self, X, autoFindNLatents=False, varianceThreshold=0.9, method=&#39;percentile&#39;, percentile=99, alpha=0.01):
    &#34;&#34;&#34;
    A utility method that calls the following model-training related methods in succession.
    
    fit(X, autoFindNLatents, varianceThreshold)
    computeMetrics(X, isTrainingData=True)
    computeThresholds(method, percentile, alpha)
    draw_monitoring_charts(title=&#39;training data&#39;)
    
    Note: Check doc string of individual methods for details on the method parameters.
    &#34;&#34;&#34;
    
    self.fit(X, autoFindNLatents, varianceThreshold)
    self.computeMetrics(X, isTrainingData=True)
    self.computeThresholds(method, percentile, alpha)
    self.draw_monitoring_charts(title=&#39;training data&#39;)
    
    return self</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Augment X, fit the model with augmented X, and apply the dimensionality reduction on augmented X.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Training data, where n_samples is the number of samples and n_features is the number of features.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X_transformed</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples-n_lags, n_components)</code></dt>
<dd>Transformed values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_transform(self, X):
    &#34;&#34;&#34;
    Augment X, fit the model with augmented X, and apply the dimensionality reduction on augmented X.

    Parameters
    ----------
    X: ndarray of shape (n_samples, n_features)
        Training data, where n_samples is the number of samples and n_features is the number of features.

    Returns
    -------
    X_transformed : ndarray of shape (n_samples-n_lags, n_components)
        Transformed values.
    &#34;&#34;&#34;
    
    # augument data and call original function
    X_aug = _augument(X, self.n_lags)
    return PCA.fit_transform(self, X_aug)</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.inverse_transform"><code class="name flex">
<span>def <span class="ident">inverse_transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform data back to its original augmented space.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples, n_components)</code></dt>
<dd>DPCS score data, where n_samples is the number of samples and n_components is the number of components retained in the DPCA nodel.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X_augmented_reconstructed</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Reconstructed augmented values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_transform(self, X):
    &#34;&#34;&#34;
    Transform data back to its original augmented space.
    
    Parameters
    ----------
    X: ndarray of shape (n_samples, n_components)
        DPCS score data, where n_samples is the number of samples and n_components is the number of components retained in the DPCA nodel.

    Returns
    -------
    X_augmented_reconstructed : ndarray of shape (n_samples, n_features)
        Reconstructed augmented values.
    &#34;&#34;&#34;
           
    return PCA.inverse_transform(self, X)</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Apply augmentation to X and then compute Score. Returns the average log-likelihood of all samples.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ll</code></strong> :&ensp;<code>float</code></dt>
<dd>Average log-likelihood of the samples under the current model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score(self, X):
    &#34;&#34;&#34;
    Apply augmentation to X and then compute Score. Returns the average log-likelihood of all samples.

    Parameters
    ----------
    X : ndarray of shape (n_samples, n_features)
        The data.

    Returns
    -------
    ll : float
        Average log-likelihood of the samples under the current model.
    &#34;&#34;&#34;
            
    # augument data and call original function
    X_aug = _augument(X, self.n_lags)
    return PCA.score(self, X_aug)</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.score_samples"><code class="name flex">
<span>def <span class="ident">score_samples</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Apply augmentation to X and then compute score for each augmented sample. Returns the log-likelihood of each sample.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ll</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples,)</code></dt>
<dd>The first n_lags entries are nan.
Log-likelihood of each augmented sample under the current model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score_samples(self, X):
    &#34;&#34;&#34;
    Apply augmentation to X and then compute score for each augmented sample. Returns the log-likelihood of each sample.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        The data.

    Returns
    -------
    ll : ndarray of shape (n_samples,)
        The first n_lags entries are nan.
        Log-likelihood of each augmented sample under the current model.
    &#34;&#34;&#34;
    
    # augument data and call original function
    X_aug = _augument(X, self.n_lags)
    return np.append(np.repeat(np.nan, self.n_lags), PCA.score_samples(self, X_aug))</code></pre>
</details>
</dd>
<dt id="PDS_DPCA.PDS_DPCA.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Apply augmentation to X and then perform dimensionality reduction.
Augmented X is projected on the first principal components previously extracted from a training set.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Data, where n_samples is the number of samples and n_features is the number of features.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X_transformed</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples-n_lags, n_components)</code></dt>
<dd>Transformed values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    &#34;&#34;&#34;
    Apply augmentation to X and then perform dimensionality reduction.
    Augmented X is projected on the first principal components previously extracted from a training set.

    Parameters
    ----------
    X: ndarray of shape (n_samples, n_features)
        Data, where n_samples is the number of samples and n_features is the number of features.

    Returns
    -------
    X_transformed : ndarray of shape (n_samples-n_lags, n_components)
        Transformed values.
    &#34;&#34;&#34;
    
    # augument data and call original function
    X_aug = _augument(X, self.n_lags)
    return PCA.transform(self, X_aug)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="PDS_DPCA.PDS_DPCA" href="#PDS_DPCA.PDS_DPCA">PDS_DPCA</a></code></h4>
<ul class="">
<li><code><a title="PDS_DPCA.PDS_DPCA.computeMetrics" href="#PDS_DPCA.PDS_DPCA.computeMetrics">computeMetrics</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.computeThresholds" href="#PDS_DPCA.PDS_DPCA.computeThresholds">computeThresholds</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.detect_abnormalities" href="#PDS_DPCA.PDS_DPCA.detect_abnormalities">detect_abnormalities</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.draw_monitoring_charts" href="#PDS_DPCA.PDS_DPCA.draw_monitoring_charts">draw_monitoring_charts</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.fit" href="#PDS_DPCA.PDS_DPCA.fit">fit</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.fit_4_monitoring" href="#PDS_DPCA.PDS_DPCA.fit_4_monitoring">fit_4_monitoring</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.fit_transform" href="#PDS_DPCA.PDS_DPCA.fit_transform">fit_transform</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.inverse_transform" href="#PDS_DPCA.PDS_DPCA.inverse_transform">inverse_transform</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.score" href="#PDS_DPCA.PDS_DPCA.score">score</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.score_samples" href="#PDS_DPCA.PDS_DPCA.score_samples">score_samples</a></code></li>
<li><code><a title="PDS_DPCA.PDS_DPCA.transform" href="#PDS_DPCA.PDS_DPCA.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>